{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":39911,"sourceType":"datasetVersion","datasetId":31296}],"dockerImageVersionId":30615,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer\nfrom torch.cuda.amp import autocast\nimport torch\nimport os\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2023-12-11T10:58:05.619528Z","iopub.execute_input":"2023-12-11T10:58:05.619945Z","iopub.status.idle":"2023-12-11T10:58:25.400857Z","shell.execute_reply.started":"2023-12-11T10:58:05.619889Z","shell.execute_reply":"2023-12-11T10:58:25.399697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/flickr-image-dataset/flickr30k_images/results.csv',delimiter='|')","metadata":{"execution":{"iopub.status.busy":"2023-12-11T11:00:06.902097Z","iopub.execute_input":"2023-12-11T11:00:06.902567Z","iopub.status.idle":"2023-12-11T11:00:07.364970Z","shell.execute_reply.started":"2023-12-11T11:00:06.902531Z","shell.execute_reply":"2023-12-11T11:00:07.363782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_ds, test_ds = train_test_split(data, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T11:08:43.875176Z","iopub.execute_input":"2023-12-11T11:08:43.875628Z","iopub.status.idle":"2023-12-11T11:08:43.937412Z","shell.execute_reply.started":"2023-12-11T11:08:43.875594Z","shell.execute_reply":"2023-12-11T11:08:43.935821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, CLIPTextModel\nfrom transformers import CLIPProcessor, CLIPModel\n\n\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\nmodel1 = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\nmodel2 = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\ntokenizer = AutoTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")","metadata":{"execution":{"iopub.status.busy":"2023-12-11T11:10:38.371715Z","iopub.execute_input":"2023-12-11T11:10:38.372151Z","iopub.status.idle":"2023-12-11T11:11:09.691381Z","shell.execute_reply.started":"2023-12-11T11:10:38.372120Z","shell.execute_reply":"2023-12-11T11:11:09.690205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CLIPTrainer(Trainer):\n    # computes loss w/o label smoothing\n    def compute_loss(self, model, inputs, return_outputs=False):\n        outputs = model(**inputs, return_loss=True)\n        return outputs[\"loss\"]\n\n    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys):\n        inputs = self._prepare_inputs(inputs)\n        with torch.no_grad():\n            if self.use_amp:\n                with autocast():\n                    loss = self.compute_loss(model, inputs)\n            else:\n                loss = self.compute_loss(model, inputs)\n        return (loss, None, None)\n    \ntrainer = CLIPTrainer(model, model.parameters(),\n                          train_dataset=train_ds,\n                          eval_dataset=test_ds,\n                          )\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-12-11T11:32:14.500418Z","iopub.execute_input":"2023-12-11T11:32:14.500803Z","iopub.status.idle":"2023-12-11T11:32:14.634637Z","shell.execute_reply.started":"2023-12-11T11:32:14.500773Z","shell.execute_reply":"2023-12-11T11:32:14.633132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n\nfor epoch in range(num_epochs):\n    for batch in your_data_loader:\n        inputs = processor(batch[\"texts\"], batch[\"images\"], return_tensors=\"pt\", padding=True, truncation=True)\n        outputs = model(**inputs)\n        loss = your_loss_function(outputs, batch[\"labels\"])\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()","metadata":{},"execution_count":null,"outputs":[]}]}